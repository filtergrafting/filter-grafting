\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{krizhevsky2012imagenet}
\citation{lotter2016deep}
\citation{graves2013speech}
\citation{zhang2015text}
\citation{li2016pruning}
\citation{molchanov2019importance}
\citation{suau2018principal}
\citation{lin2018holistic}
\citation{meng2020filter}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{li2016pruning}
\citation{hinton2015distilling}
\citation{zhang2018deep}
\citation{prakash2019repr}
\citation{han2015learning}
\citation{han2015deep}
\citation{guo2016dynamic}
\citation{liu2017learning}
\citation{lebedev2016fast}
\citation{wen2016learning}
\citation{li2016pruning}
\citation{zhuo2018scsp}
\citation{suau2018principal}
\citation{wang2018exploring}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces An illustration of the difference between filter pruning and filter grafting. For filter grafting, we graft external information into invalid filters without changing the model structure. (best viewed in color)}}{2}{figure.1}\protected@file@percent }
\newlabel{figure: pruning&grafting}{{1}{2}{An illustration of the difference between filter pruning and filter grafting. For filter grafting, we graft external information into invalid filters without changing the model structure. (best viewed in color)}{figure.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The difference between filter grafting and other learning methods}}{2}{table.1}\protected@file@percent }
\newlabel{Related_Work_Difference}{{1}{2}{The difference between filter grafting and other learning methods}{table.1}{}}
\citation{hinton2015distilling}
\citation{zhang2018deep}
\citation{bucilua2006model}
\citation{hinton2015distilling}
\citation{tang2016recurrent}
\citation{lopes2017data}
\citation{mirzadeh2019improved}
\citation{cho2019efficacy}
\citation{prakash2019repr}
\citation{webb1997decision}
\citation{li2012network}
\citation{hu2020exploiting}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}The Proposed Approach}{3}{section.3}\protected@file@percent }
\newlabel{sec_3}{{3}{3}{The Proposed Approach}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Information Source for Grafting}{3}{subsection.3.1}\protected@file@percent }
\newlabel{sec_3_1}{{3.1}{3}{Information Source for Grafting}{subsection.3.1}{}}
\citation{kumar2017weight}
\citation{he2015delving}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Noise as Scions}{4}{subsubsection.3.1.1}\protected@file@percent }
\newlabel{sec_3_1_1}{{3.1.1}{4}{Noise as Scions}{subsubsection.3.1.1}{}}
\newlabel{noise_decrease}{{1}{4}{Noise as Scions}{equation.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Internal Filters as Scions}{4}{subsubsection.3.1.2}\protected@file@percent }
\newlabel{sec_3_1_2}{{3.1.2}{4}{Internal Filters as Scions}{subsubsection.3.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Grafting internal filters. We first sort the filters by $l_{1}$ norm, then graft the weights from filters with larger $l_{1}$ norm into filters with smaller $l_{1}$ norm (best viewed in color).}}{4}{figure.2}\protected@file@percent }
\newlabel{figure: inside_grafting}{{2}{4}{Grafting internal filters. We first sort the filters by $l_{1}$ norm, then graft the weights from filters with larger $l_{1}$ norm into filters with smaller $l_{1}$ norm (best viewed in color)}{figure.2}{}}
\newlabel{theo1}{{1}{4}{Internal Filters as Scions}{theorem.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}External Filters as Scions}{5}{subsubsection.3.1.3}\protected@file@percent }
\newlabel{sec_3_1_3}{{3.1.3}{5}{External Filters as Scions}{subsubsection.3.1.3}{}}
\newlabel{weighting}{{2}{5}{External Filters as Scions}{equation.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Criterions for Calculating Information of Filters and Layers}{5}{subsection.3.2}\protected@file@percent }
\newlabel{sec_3_2}{{3.2}{5}{Criterions for Calculating Information of Filters and Layers}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}$L_{1}$ norm }{5}{subsubsection.3.2.1}\protected@file@percent }
\newlabel{sec_3_2_1}{{3.2.1}{5}{$L_{1}$ norm}{subsubsection.3.2.1}{}}
\citation{li2016pruning}
\citation{Ye2018Rethinking}
\citation{Yang2018Soft}
\citation{shwartz2017opening}
\citation{cheng2019utilizing}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Grafting between two networks. Each network receives the information from the other network (best viewed in color).}}{6}{figure.3}\protected@file@percent }
\newlabel{figure: mutual_grafting}{{3}{6}{Grafting between two networks. Each network receives the information from the other network (best viewed in color)}{figure.3}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Entropy-based Multiple Networks Grafting}}{6}{algorithm.1}\protected@file@percent }
\newlabel{algorithm2}{{1}{6}{$L_{1}$ norm}{algorithm.1}{}}
\newlabel{l1 norm}{{3}{6}{$L_{1}$ norm}{equation.3.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Entropy}{6}{subsubsection.3.2.2}\protected@file@percent }
\newlabel{sec_3_2_2}{{3.2.2}{6}{Entropy}{subsubsection.3.2.2}{}}
\newlabel{binning}{{4}{7}{Entropy}{equation.3.4}{}}
\newlabel{info_layer_pre}{{5}{7}{Entropy}{equation.3.5}{}}
\newlabel{info_layer}{{6}{7}{Entropy}{equation.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Adaptive Weighting in Grafting}{7}{subsection.3.3}\protected@file@percent }
\newlabel{sec_3_3}{{3.3}{7}{Adaptive Weighting in Grafting}{subsection.3.3}{}}
\newlabel{coefficient}{{7}{7}{Adaptive Weighting in Grafting}{equation.3.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Extending Grafting to Multiple Networks}{7}{subsection.3.4}\protected@file@percent }
\newlabel{sec_3_4}{{3.4}{7}{Extending Grafting to Multiple Networks}{subsection.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Cultivating the Grafted Network (Grafting+)}{7}{subsection.3.5}\protected@file@percent }
\newlabel{sec_3_5}{{3.5}{7}{Cultivating the Grafted Network (Grafting+)}{subsection.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The adaptive coefficient in grafting process.}}{8}{figure.4}\protected@file@percent }
\newlabel{figure: adaptive_w}{{4}{8}{The adaptive coefficient in grafting process}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Grafting with multiple networks. The network $M_{k}$ accepts information from $M_{k-1}$. (best viewed in color)}}{8}{figure.5}\protected@file@percent }
\newlabel{figure: multiple_grafting}{{5}{8}{Grafting with multiple networks. The network $M_{k}$ accepts information from $M_{k-1}$. (best viewed in color)}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces This is the overall framework of grafting+. KD loss is added to the student's cross-entropy loss. (best viewed in color)}}{9}{figure.6}\protected@file@percent }
\newlabel{figure: grafting+}{{6}{9}{This is the overall framework of grafting+. KD loss is added to the student's cross-entropy loss. (best viewed in color)}{figure.6}{}}
\newlabel{eq_distill}{{11}{9}{Cultivating the Grafted Network (Grafting+)}{equation.3.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{9}{section.4}\protected@file@percent }
\newlabel{sec:exp}{{4}{9}{Experiments}{section.4}{}}
\citation{cifar10}
\citation{zhang2018deep}
\citation{prakash2019repr}
\citation{zhang2018deep}
\citation{prakash2019repr}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Information Sources and Criterion}{10}{subsection.4.1}\protected@file@percent }
\newlabel{sec_4_1}{{4.1}{10}{Information Sources and Criterion}{subsection.4.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Comparison of different scion sources. }}{10}{table.2}\protected@file@percent }
\newlabel{table:4_1}{{2}{10}{Comparison of different scion sources}{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Comparison of grafting by $l_{1}$ norm \& entropy. }}{10}{table.3}\protected@file@percent }
\newlabel{table:norm_entropy}{{3}{10}{Comparison of grafting by $l_{1}$ norm \& entropy}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Evaluating Grafting on Different Datasets}{10}{subsection.4.2}\protected@file@percent }
\newlabel{sec_4_2}{{4.2}{10}{Evaluating Grafting on Different Datasets}{subsection.4.2}{}}
\citation{deng2009imagenet}
\citation{deng2018image}
\citation{wei2018person}
\citation{lin2018multi}
\citation{wang2018transferable}
\citation{market1505}
\citation{Ristani2016Performance}
\citation{zheng2017unlabeled}
\citation{zhou2019osnet}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Comparion of filter grafting with other learning methods. `--' denotes the result is not reported in the corresponding paper.}}{11}{table.4}\protected@file@percent }
\newlabel{table:4_3}{{4}{11}{Comparion of filter grafting with other learning methods. `--' denotes the result is not reported in the corresponding paper}{table.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Grafting with multiple networks (MobileNetV2). }}{11}{table.5}\protected@file@percent }
\newlabel{table:4_4}{{5}{11}{Grafting with multiple networks (MobileNetV2)}{table.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Grafting on ImageNet Dataset}}{12}{table.6}\protected@file@percent }
\newlabel{table:Imagenet}{{6}{12}{Grafting on ImageNet Dataset}{table.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Grafting on ReID Task}}{12}{table.7}\protected@file@percent }
\newlabel{table:ReID}{{7}{12}{Grafting on ReID Task}{table.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Ratio of filters whose $l_{1}$ norm under some threshold. }}{12}{figure.7}\protected@file@percent }
\newlabel{figure: l1_threshold}{{7}{12}{Ratio of filters whose $l_{1}$ norm under some threshold}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  Entropy and accuracy of the baseline network and grafted network. The network's information is defined as the sum of all the layers' entropy in a \textbf  {single} network. The $x$ axis denotes the number of networks parallelly trained in grafting algorithm. }}{13}{figure.8}\protected@file@percent }
\newlabel{figure: entropy_compare}{{8}{13}{Entropy and accuracy of the baseline network and grafted network. The network's information is defined as the sum of all the layers' entropy in a \textbf {single} network. The $x$ axis denotes the number of networks parallelly trained in grafting algorithm}{figure.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces This table records the density of valid filters and invalid filters for each method. }}{13}{table.8}\protected@file@percent }
\newlabel{table:branches}{{8}{13}{This table records the density of valid filters and invalid filters for each method}{table.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Cultivating with Distillation (Grafting+)}{13}{subsection.4.3}\protected@file@percent }
\newlabel{sec_4_3}{{4.3}{13}{Cultivating with Distillation (Grafting+)}{subsection.4.3}{}}
\citation{yang2019snapshot}
\citation{meng2020filter}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces This figure shows the filter weight of each layer. Each filter is represented by a colored cube which is relational to the value of its $l_{1}$ norm. The black cubes represent the filters whose $l_{1}$ norm are very close to 0. (best viewed in color)}}{14}{figure.9}\protected@file@percent }
\newlabel{figure:bgd}{{9}{14}{This figure shows the filter weight of each layer. Each filter is represented by a colored cube which is relational to the value of its $l_{1}$ norm. The black cubes represent the filters whose $l_{1}$ norm are very close to 0. (best viewed in color)}{figure.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces This table records the accuracy of CIFAR-100 for each method. Grafting trains two student networks simultaneously while distillation maintains one teacher network and one student network. Grafting+ involves one teacher network and two students networks. For each method, we use the single student network for testing. }}{15}{table.9}\protected@file@percent }
\newlabel{table:grafting+_cifar}{{9}{15}{This table records the accuracy of CIFAR-100 for each method. Grafting trains two student networks simultaneously while distillation maintains one teacher network and one student network. Grafting+ involves one teacher network and two students networks. For each method, we use the single student network for testing}{table.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces This table records the average $l_{1}$ norm of valid filters and invalid filters for each method. The backbone is ResNet-56. }}{15}{table.10}\protected@file@percent }
\newlabel{table:branches_2}{{10}{15}{This table records the average $l_{1}$ norm of valid filters and invalid filters for each method. The backbone is ResNet-56}{table.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Evaluating the Grafting+}{15}{subsection.4.4}\protected@file@percent }
\newlabel{sec_4_4}{{4.4}{15}{Evaluating the Grafting+}{subsection.4.4}{}}
\bibstyle{unsrt}
\bibdata{references}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces This figure depicts the validation accuracy of each methods on CIFAR-100 datasets. The network is MobileNetV2. (best viewed in color)}}{16}{figure.10}\protected@file@percent }
\newlabel{figure: accuracy_curve}{{10}{16}{This figure depicts the validation accuracy of each methods on CIFAR-100 datasets. The network is MobileNetV2. (best viewed in color)}{figure.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces This table records the student accuracy with number of teachers and students in grafting+ framework. The network structures for teacher and student are ResNet-110 and MobileNetV2, respectively.}}{16}{table.11}\protected@file@percent }
\newlabel{table:number_T_S}{{11}{16}{This table records the student accuracy with number of teachers and students in grafting+ framework. The network structures for teacher and student are ResNet-110 and MobileNetV2, respectively}{table.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{16}{section.5}\protected@file@percent }
\bibcite{krizhevsky2012imagenet}{1}
\bibcite{lotter2016deep}{2}
\bibcite{graves2013speech}{3}
\bibcite{zhang2015text}{4}
\bibcite{li2016pruning}{5}
\bibcite{molchanov2019importance}{6}
\bibcite{suau2018principal}{7}
\bibcite{lin2018holistic}{8}
\bibcite{meng2020filter}{9}
\bibcite{hinton2015distilling}{10}
\bibcite{zhang2018deep}{11}
\bibcite{prakash2019repr}{12}
\bibcite{han2015learning}{13}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces This figure shows the filters' $l_{1}$ norm of Conv3 layer for baseline and grafting+. (best viewed in color)}}{17}{figure.11}\protected@file@percent }
\newlabel{figure:filter_norm}{{11}{17}{This figure shows the filters' $l_{1}$ norm of Conv3 layer for baseline and grafting+. (best viewed in color)}{figure.11}{}}
\bibcite{han2015deep}{14}
\bibcite{guo2016dynamic}{15}
\bibcite{liu2017learning}{16}
\bibcite{lebedev2016fast}{17}
\bibcite{wen2016learning}{18}
\bibcite{zhuo2018scsp}{19}
\bibcite{wang2018exploring}{20}
\bibcite{bucilua2006model}{21}
\bibcite{tang2016recurrent}{22}
\bibcite{lopes2017data}{23}
\bibcite{mirzadeh2019improved}{24}
\bibcite{cho2019efficacy}{25}
\bibcite{webb1997decision}{26}
\bibcite{li2012network}{27}
\bibcite{hu2020exploiting}{28}
\bibcite{kumar2017weight}{29}
\bibcite{he2015delving}{30}
\bibcite{Ye2018Rethinking}{31}
\bibcite{Yang2018Soft}{32}
\bibcite{shwartz2017opening}{33}
\bibcite{cheng2019utilizing}{34}
\bibcite{cifar10}{35}
\bibcite{deng2009imagenet}{36}
\bibcite{deng2018image}{37}
\bibcite{wei2018person}{38}
\bibcite{lin2018multi}{39}
\bibcite{wang2018transferable}{40}
\bibcite{market1505}{41}
\bibcite{Ristani2016Performance}{42}
\bibcite{zheng2017unlabeled}{43}
\bibcite{zhou2019osnet}{44}
\bibcite{yang2019snapshot}{45}
